{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncNrAEpzmp8S"
   },
   "source": [
    "# Tutorial 08 - Object Detection with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DevCube GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start, you have to find a GPU on the system that is not heavily used by other users. Otherwise you cannot initialize your neural network.\n",
    "\n",
    "\n",
    "**Hint:** the command is **nvidia-smi**, just in case it is displayed above in two lines because of a line break.\n",
    "\n",
    "As a result you get a summary of the GPUs available in the system, their current memory usage (in MiB for megabytes), and their current utilization (in %). There should be six or eight GPUs listed and these are numbered 0 to n-1 (n being the number of GPUs). The GPU numbers (ids) are quite at the beginning of each GPU section and their numbers increase from top to bottom by 1.\n",
    "\n",
    "Find a GPU where the memory usage is low. For this purpose look at the memory usage, which looks something like '365MiB / 16125MiB'. The first value is the already used up memory and the second value is the total memory of the GPU. Look for a GPU where there is a large difference between the first and the second value.\n",
    "\n",
    "**Remember the GPU id and write it in the next line instead of the character X.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**YOLO needs a lot of GPU memory. If the notebook does not work on a certain GPU, then try a GPU with 16GB of memory.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 30 12:25:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 33%   30C    P8    17W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 5000     On   | 00000000:24:00.0 Off |                  Off |\n",
      "| 33%   28C    P8     7W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 5000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   28C    P8     6W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 5000     On   | 00000000:61:00.0 Off |                  Off |\n",
      "| 33%   25C    P8     7W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 5000     On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 33%   24C    P8    10W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 5000     On   | 00000000:A1:00.0 Off |                  Off |\n",
      "| 33%   23C    P8    13W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Quadro RTX 5000     On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 33%   24C    P8    10W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Quadro RTX 5000     On   | 00000000:E1:00.0 Off |                  Off |\n",
      "| 33%   25C    P8     7W / 230W |      1MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose one GPU\n",
    "\n",
    "**The following code is very important and must always be executed before using TensorFlow in the exercises, so that only one GPU is used and that it is set in a way that not all its memory is used at once. Otherwise, the other students will not be able to work with this GPU.**\n",
    "\n",
    "The following program code imports the TensorFlow library for Deep Learning and outputs the version of the library.\n",
    "\n",
    "Then, TensorFlow is configured to only see the one GPU whose number you wrote in the above cell (USE_GPU = X) instead of the X.\n",
    "\n",
    "Finally, the GPU is set so that it does not immediately reserve all memory, but only uses more memory when needed. \n",
    "\n",
    "(The comments within the code cell explains a bit of what is happening if you are interested to better understand it. See also the documentation of TensorFlow for an explanation of the used methods.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:7 GPU\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial object detection with YOLO\n",
    "\n",
    "This tutorial will introduce you to object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: [Redmon et al., 2016](https://arxiv.org/abs/1506.02640) and [Redmon and Farhadi, 2016](https://arxiv.org/abs/1612.08242). \n",
    "\n",
    "### LEARNING OBJECTIVES:\n",
    "\n",
    "- Use object detection on a car detection dataset\n",
    "\n",
    "+ Recognizing Multiple Images with YOLO Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Task Description\n",
    "\n",
    "You are working on a self-driving car. As a critical component of this project, you'd like to first build a car detection system. To collect data, you've mounted a camera to the hood (meaning the front) of the car, which takes pictures of the road ahead every few seconds while you drive around. You've gathered all these images into a folder and have labelled them by drawing bounding boxes around every car you found. Here's an example of what your bounding boxes look like.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/box_label.png\" style=\"width:500px;height:250;\">\n",
    "</center>\n",
    "<caption><center>  **Figure 1** : **Definition of a box**<br> </center></caption>\n",
    "\n",
    "If you have 80 classes that you want the object detector to recognize, you can represent the class label $c$ either as an integer from 1 to 80, or as an 80-dimensional vector (with 80 numbers) one component of which is 1 and the rest of which are 0.  \n",
    "\n",
    "In this exercise, you will learn how \"You Only Look Once\" (YOLO) performs object detection, and then apply it to car detection. Because the YOLO model is very computationally expensive to train, we will load pre-trained weights for you to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Intro YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You Only Look Once\" (YOLO) is a popular algorithm because it achieves high accuracy while also being able to run in real-time. This algorithm \"only looks once\" at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes.\n",
    "\n",
    "### Model details\n",
    "\n",
    "#### Inputs and outputs\n",
    "- The **input** is a batch of images, and each image has the shape (m, 608, 608, 3)\n",
    "- The **output** is a list of bounding boxes along with the recognized classes. Each bounding box is represented by 6 numbers $(p_c, b_x, b_y, b_h, b_w, c)$ as explained above. If you expand $c$ into an 80-dimensional vector, each bounding box is then represented by 85 numbers. \n",
    "\n",
    "#### Anchor Boxes\n",
    "* Anchor boxes are chosen by exploring the training data to choose reasonable height/width ratios that represent the different classes.  For the example below, 5 anchor boxes were chosen  (to cover the 80 classes), and stored in the file './model_data/yolo_anchors.txt'\n",
    "* The dimension for anchor boxes is the second to last dimension in the encoding: $(m, n_H,n_W,anchors,classes)$.\n",
    "* The YOLO architecture is: IMAGE (m, 608, 608, 3) -> DEEP CNN -> ENCODING (m, 19, 19, 5, 85).  \n",
    "\n",
    "\n",
    "#### Encoding\n",
    "Let's look in greater detail at what this encoding represents. \n",
    "<center> \n",
    "<img src=\"images/architecture.png\" style=\"width:800px;height:500;\">\n",
    "     </center>\n",
    "<caption><center> <u> **Figure 2** </u>: **Encoding architecture for YOLO**<br> </center></caption>\n",
    "\n",
    "If the center/midpoint of an object falls into a grid cell, that grid cell is responsible for detecting that object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using 5 anchor boxes, each of the 19 x19 cells thus encodes information about 5 boxes. Anchor boxes are defined only by their width and height.\n",
    "\n",
    "For simplicity, we will flatten the last two last dimensions of the shape (19, 19, 5, 85) encoding. So the output of the Deep CNN is (19, 19, 425).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/flatten.png\" style=\"width:700px;height:400;\">\n",
    "</center>\n",
    "<caption><center> <u> **Figure 3** </u>: **Flattening the last two last dimensions**<br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class score\n",
    "\n",
    "Now, for each box (of each cell) we will compute the following element-wise product and extract a probability that the box contains a certain class.  \n",
    "The class score is $score_{c,i} = p_{c} \\times c_{i}$: the probability that there is an object $p_{c}$ times the probability that the object is a certain class $c_{i}$.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/probability_extraction.png\" style=\"width:700px;height:400;\">\n",
    "</center>\n",
    "<caption><center> <u> **Figure 4** </u>: **Find the class detected by each box**<br> </center></caption>\n",
    "\n",
    "##### Example of figure 4\n",
    "* In figure 4, let's say for box 1 (cell 1), the probability that an object exists is $p_{1}=0.60$.  So there's a 60% chance that an object exists in box 1 (cell 1).  \n",
    "* The probability that the object is the class \"category 3 (a car)\" is $c_{3}=0.73$.  \n",
    "* The score for box 1 and for category \"3\" is $score_{1,3}=0.60 \\times 0.73 = 0.44$.  \n",
    "* Let's say we calculate the score for all 80 classes in box 1, and find that the score for the car class (class 3) is the maximum.  So we'll assign the score 0.44 and class \"3\" to this box \"1\".\n",
    "\n",
    "#### Visualizing classes\n",
    "Here's one way to visualize what YOLO is predicting on an image:\n",
    "- For each of the 19x19 grid cells, find the maximum of the probability scores (taking a max across the 80 classes, one maximum for each of the 5 anchor boxes).\n",
    "- Color that grid cell according to what object that grid cell considers the most likely.\n",
    "\n",
    "Doing this results in this picture: \n",
    "\n",
    "<center> \n",
    "<img src=\"images/proba_map.png\" style=\"width:300px;height:300;\"> \n",
    "</center>\n",
    "<caption><center><u> **Figure 5** </u>: Each one of the 19x19 grid cells is colored according to which class has the largest predicted probability in that cell.<br></center></caption>  \n",
    "  \n",
    "  \n",
    "Note that this visualization isn't a core part of the YOLO algorithm itself for making predictions; it's just a nice way of visualizing an intermediate result of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing bounding boxes\n",
    "Another way to visualize YOLO's output is to plot the bounding boxes that it outputs. Doing that results in a visualization like this:  \n",
    "\n",
    "<center><img src=\"images/anchor_map.png\" style=\"width:200px;height:200;\"></center>\n",
    "<caption><center>  **Figure 6** : Each cell gives you 5 boxes. In total, the model predicts: 19x19x5 = 1805 boxes just by looking once at the image (one forward pass through the network)! Different colors denote different classes. <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fU9UhAxTmp8S",
    "outputId": "3becf193-0a32-4bea-d087-95258f817148"
   },
   "outputs": [],
   "source": [
    "# helper function to formate time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSKZqD1Mmp-C"
   },
   "source": [
    "## 03 - Performing Car Detection with YOLO v3\n",
    "\n",
    "\n",
    "\n",
    "### Using YOLO in Python\n",
    "\n",
    "To make use of YOLO in Python, you have several options:\n",
    "\n",
    "* **[DarkNet](https://pjreddie.com/darknet/yolo/)** - The original implementation of YOLO, written in C.\n",
    "* **[yolov3-tf2](https://github.com/zzh8829/yolov3-tf2)** - An unofficial Python package that implements YOLO in Python, using TensorFlow 2.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ym5_juokofQl"
   },
   "source": [
    "### Installing YoloV3-TF2\n",
    "\n",
    "YoloV3-TF2 is not available directly through either PIP or CONDA. Therefore you need to go through several steps to install it.  This section describes the process of installing YoloV3-TF2.    For a local install, you must perform these steps only once for your virtual Python environment.  If you are installing locally, make sure to install to the same virtual environment that you created for this course.  The following command installs YoloV3-TF2 directly from it's GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "Fd_Dgjm4iLyh",
    "outputId": "baf1adfa-ca47-4e4b-efb9-b640a5e2d2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/zzh8829/yolov3-tf2.git@master\n",
      "  Cloning https://github.com/zzh8829/yolov3-tf2.git (to revision master) to /tmp/pip-req-build-q7zuqijk\n",
      "  Running command git clone -q https://github.com/zzh8829/yolov3-tf2.git /tmp/pip-req-build-q7zuqijk\n",
      "Building wheels for collected packages: yolov3-tf2\n",
      "  Building wheel for yolov3-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yolov3-tf2: filename=yolov3_tf2-0.1-py3-none-any.whl size=9187 sha256=e4149f679ab1acc30be78816b952c5ea97fcb49a8cd9ef7634b60067b6564fa4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6bolfhu7/wheels/dc/40/57/f6ce9c0aa58da78f10d29a11476132dbf0a616bb92826be28f\n",
      "Successfully built yolov3-tf2\n",
      "Installing collected packages: yolov3-tf2\n",
      "Successfully installed yolov3-tf2-0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install git+https://github.com/zzh8829/yolov3-tf2.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use of YoloV3-TF2 we need the following files:\n",
    "\n",
    "* **yolov3.weights** - These are the pre-trained weights provided by the author of YOLO.\n",
    "* **coco.names** - The names of the 80 items that the **yolov3.weights** neural network was trained to recognize.\n",
    "* **yolov3.tf** - The YOLO weights converted to a format that TensorFlow can use directly.\n",
    "\n",
    "These are located at the ususal location **'coursematerial/GIS/YOLO'**. (You do **not** need to copy in your working directory.)\n",
    "\n",
    "Researchers have trained YOLO on a variety of different computer image datasets.  The version of YOLO weights used in this course is from the dataset Common Objects in Context (COCO). [[Cite: lin2014microsoft]](https://arxiv.org/abs/1405.0312) This dataset contains images labeled into 80 different classes. \n",
    "\n",
    "YOLO was also adapted for mobile devices by creating the YOLO Tiny pre-trained weights that use a much smaller convolutional neural network and still achieve acceptable levels of quality.  Though YoloV3-TF2 can work with either YOLO Tiny or regular YOLO we are not using the tiny weights for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ThxRyzT1iLyj",
    "outputId": "a9efc33a-8e09-4d1d-f449-70a23e499a68"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = str(Path.home()) + r'/coursematerial/GIS/YOLO/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering Weights\n",
    "\n",
    "In this tutorial we use trained weights for our YOLO networks.  It can take considerable time to train a YOLO network from scratch.  If you would like to train a YOLO network to recognize images other than the COLO provided images, then you may need to train your own YOLO information.  If training from scratch is something you need to do, there is further information on this at the YoloV3-TF2 GitHub repository.\n",
    "\n",
    "The weights provided by the original authors of YOLO is not directly compatible with TensorFlow.  Because of this, the provided YOLO  weights have been convert the into a TensorFlow compatible format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion script is no longer needed once this script converts the YOLO weights have to a TensorFlow format.  Because this executable file resides in the same directory as the course files, we delete it at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename_classes =  os.path.join(ROOT,'coco.names')\n",
    "filename_converted_weights = os.path.join(ROOT,'yolov3.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the files needed for YOLO, we are ready to use it to recognize components of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYOvD3M7ofQl"
   },
   "source": [
    "### Running YOLO (Darkflow)\n",
    "\n",
    "The YoloV3-TF2 library can easily integrate with Python applications.  The initialization of the library consists of three steps.  First, it is essential to import all of the needed packages for the library.  Next, the Python program must define all of the YOLO configurations through the Keras flags architecture. The Keras flag system primarily works from the command line; however, it also allows configuration programmatically in an application.  For this example, we configure the package programmatically.  Finally, we must scan available devices so that our application takes advantage of any GPUs.   The following code performs all three of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MY3gVyidmp-K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3_tf2.models import (YoloV3, YoloV3Tiny)\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
    "from yolov3_tf2.utils import draw_outputs\n",
    "import sys\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "\n",
    "# Flags are used to define several options for YOLO.\n",
    "flags.DEFINE_string('classes', filename_classes, 'path to classes file')\n",
    "flags.DEFINE_string('weights', filename_converted_weights, 'path to weights file')\n",
    "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
    "flags.DEFINE_integer('size', 416, 'resize images to')\n",
    "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
    "flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n",
    "FLAGS([sys.argv[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that Keras flags can only be defined once. If you are going to classify more than one image, make sure that you do not define the flags additional times.\n",
    "\n",
    "The following code initializes a YoloV3-TF2 classification object.  The weights are loaded, and the object is ready for use as the **yolo** variable.  It is not necessary to reload the weights and obtain a new **yolo** variable for each classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example does not use the \"Tiny version\"\n",
    "if FLAGS.tiny:\n",
    "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n",
    "else:\n",
    "    yolo = YoloV3(classes=FLAGS.num_classes)\n",
    "\n",
    "# Load weights and classes\n",
    "yolo.load_weights(FLAGS.weights).expect_partial()\n",
    "print('weights loaded')\n",
    "\n",
    "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
    "print('classes loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain an image to classify. For this example, the program loads an image from the coursematerial folder.  YoloV3-TF2 expects that the image is in the format of a Numpy array.  An image file, such as JPEG or PNG, is converted into this raw Numpy format by calling the TensorFlow **decode_image** function.  YoloV3-TF2 can obtain images from other sources, so long as the program first decodes them to raw Numpy format.  The following code obtains the image in this format.\n",
    "\n",
    "Images are provided in the **'coursematerial/GIS/YOLO/images'** folder that you can directly access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5ilnhhNiLyq"
   },
   "outputs": [],
   "source": [
    "image =  os.path.join(ROOT, r'images/0035.jpg')\n",
    "content = tf.io.read_file(image)\n",
    "img_raw = tf.image.decode_image(content, channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can classify the image that was just loaded.  The program should preprocess the image so that it is the size expected by YoloV3-TF2.  Your program also sets the confidence threshold at this point.  Any sub-image recognized with confidence below this value is not returned by YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gfortIjsiLyr",
    "outputId": "b8f48006-0537-4561-f46b-3900b14c3bd1"
   },
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "img = tf.expand_dims(img_raw, 0)\n",
    "img = transform_images(img, FLAGS.size)\n",
    "\n",
    "# Desired threshold (any sub-image below this confidence \n",
    "# level will be ignored.)\n",
    "FLAGS.yolo_score_threshold = 0.5\n",
    "\n",
    "# Recognize and report results\n",
    "t1 = time.time()\n",
    "boxes, scores, classes, nums = yolo(img)\n",
    "t2 = time.time()\n",
    "print(f\"Prediction time: {hms_string(t2 - t1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that prediction time for your first image takes the longest. The next images take much less time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the **yolo** class instantiated here is a callable object, which means that it can fill the role of both an object and a function. Acting as a function, *yolo* returns three arrays named **boxes**, **scores**, and **classes** that are of the same length.  The function returns all sub-images found with a score above the minimum threshold.  Additionally, the **yolo** function returns an array named called **nums**. The first element of the **nums** array specifies how many sub-images YOLO found to be above the score threshold.\n",
    "\n",
    "* **boxes** - The bounding boxes for each of the sub-images detected in the image sent to YOLO.\n",
    "* **scores** - The confidence for each of the sub-images detected.\n",
    "* **classes** - An array index to the string class names for each of the items. These are COCO names such as \"person\" or \"dog.\" \n",
    "* **nums** - The number of images above the threshold.\n",
    "\n",
    "Your program should use these values to perform whatever actions you wish as a result of the input image.  The following code simply displays the images detected above the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "tzXAESCLiLyt",
    "outputId": "5e5ad983-482f-436a-8f8b-4c3353f392fc"
   },
   "outputs": [],
   "source": [
    "print('detections:')\n",
    "for i in range(nums[0]):\n",
    "    cls = class_names[int(classes[0][i])]\n",
    "    score = np.array(scores[0][i])\n",
    "    box = np.array(boxes[0][i])\n",
    "    print(f\"\\t{cls}, {score}, {box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program should use these values to perform whatever actions you wish as a result of the input image.  The following code simply displays the images detected above the threshold.\n",
    "\n",
    "YoloV3-TF2 includes a function named **draw_outputs** that allows the sub-image detections to visualized.  The following image shows the output of the draw_outputs function.  You might have first seen YOLO demonstrated as an image with boxes and labels around the sub-images. A program can produce this output with the arrays returned by the **yolo** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "vmOhAGktiLyv",
    "outputId": "139b441a-2d26-451c-d211-65693e6dcf7b"
   },
   "outputs": [],
   "source": [
    "# Display image using YOLO library's built in function\n",
    "img = img_raw.numpy()\n",
    "img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n",
    "#cv2.imwrite(FLAGS.output, img) # Save the image\n",
    "display(Image.fromarray(img, 'RGB')) # Display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **yolo** class returns everything as TensorFlow tensor where the first dimension (in our example) is always 1 as there is one image. That is the reason you have to use, e.g., classes[0] to get to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'classes: {classes.shape}')\n",
    "print(f'boxes:   {boxes.shape}')\n",
    "print(f'scores:  {scores.shape}')\n",
    "print(f'nums:    {nums.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classes:\\n {classes[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scores:\\n {scores[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ideas to play around with YOLO and practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try a different image (maybe one with different objects).\n",
    "2. Modify the code above (or write new code in the cells below) in such a way that a random image from the images location is selected and run the YOLO prediction.\n",
    "3. Use a different score threshold. What happens?\n",
    "4. Try out your own street image and detect the represented objects with YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "t81_558_class_06_5_yolo-fix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
