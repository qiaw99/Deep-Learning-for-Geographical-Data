{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 - PointNet for 3D Point Cloud Classification\n",
    "\n",
    "This exercises is about implementing the PointNet architecture for point-wise classification of a 3D point cloud by considering local neighborhoods of 20 points around each point to classify. (The point itself is also used, so that in the end there are 21 points as input to PointNet.)\n",
    "\n",
    "The datasets are provided as 3-dimensional tensors, where for each point of the point cloud (D), the 21 points (N=21) are given, with their 3D coordinates (Dx21x3). The training dataset of ISPRS is used for training, and the evaluation dataset for testing (predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:4 GPU\n",
      "\n",
      "Keras version: 2.4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# Import Keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the installed Keras version\n",
    "print(f'\\nKeras version: {keras.__version__}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_colorized_point_cloud(xyz, y, filename):\n",
    "\n",
    "    color_map = np.array([\n",
    "        [255, 255, 125],\n",
    "        [  0, 255, 255],\n",
    "        [255, 255, 255],\n",
    "        [255, 255,   0],\n",
    "        [  0, 255, 125],\n",
    "        [  0,   0, 255],\n",
    "        [  0, 125, 255],\n",
    "        [125, 255,   0],\n",
    "        [  0, 255,   0]])\n",
    "    \n",
    "    u, inverses = np.unique(y, return_inverse=True)    \n",
    "    \n",
    "    colors = color_map[inverses]\n",
    "    \n",
    "    df = pd.DataFrame(xyz, columns=['x', 'y', 'z'])    \n",
    "\n",
    "    df['red'] = pd.Series(data=colors[:,0], name='red')\n",
    "    df['green'] = pd.Series(data=colors[:,1], name='green')\n",
    "    df['blue'] = pd.Series(data=colors[:,2], name='blue')\n",
    "    \n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    print(f'Saved \"{filename}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "The dataset used in this exercise consists of a large number (753.876 for training) of very small point clouds with 21 points each. They are the result of taking each point from the original point cloud together with their 20 neighbor points. And they form the input to the PointNet neural network to predict the class for the one \"central\" point.\n",
    "\n",
    "The point clouds are centered by their \"central\" point, so that the x,y-coordinates of the central point is in the origin of the coordinates system. And the z-coordinate of this point is the elevation over the terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root_dir = str(Path.home()) + r'/coursematerial/GIS/ISPRS/PointNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read \"Vaihingen3D_Training_PointNet_XYZ.npy\" feature matrix of shape (753876, 21, 3)\n",
      "Successfully read \"Vaihingen3D_Training_PointNet_Labels.npy\" label vector of shape  (753876, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename_xyz = 'Vaihingen3D_Training_PointNet_XYZ.npy'\n",
    "filename_lab = 'Vaihingen3D_Training_PointNet_Labels.npy'\n",
    "\n",
    "# Load the point clouds with the  x,y,z-values\n",
    "X = np.load(os.path.join(root_dir, filename_xyz))\n",
    "print(f'Successfully read \"{filename_xyz}\" feature matrix of shape {X.shape}')\n",
    "\n",
    "# Load labels\n",
    "y = np.load(os.path.join(root_dir, filename_lab))\n",
    "print(f'Successfully read \"{filename_lab}\" label vector of shape  {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.02],\n",
       "       [ 0.07,  0.15,  0.  ],\n",
       "       [ 0.04, -0.25, -0.02],\n",
       "       [-0.3 , -0.2 , -0.03],\n",
       "       [ 0.36, -0.1 ,  0.  ],\n",
       "       [-0.01, -0.38, -0.01],\n",
       "       [ 0.01,  0.38, -0.02],\n",
       "       [ 0.38,  0.27,  0.  ],\n",
       "       [ 0.1 ,  0.55, -0.01],\n",
       "       [ 0.35, -0.48,  0.02],\n",
       "       [-0.32, -0.56, -0.03],\n",
       "       [ 0.01, -0.66, -0.02],\n",
       "       [ 0.39,  0.64, -0.01],\n",
       "       [-0.02, -0.77,  0.02],\n",
       "       [ 0.34, -0.85,  0.03],\n",
       "       [ 0.4 ,  1.01, -0.01],\n",
       "       [ 1.15,  0.02,  0.  ],\n",
       "       [ 1.13, -0.35,  0.01],\n",
       "       [ 1.18,  0.39, -0.01],\n",
       "       [ 1.25, -0.14, -0.01],\n",
       "       [ 1.1 , -0.71,  0.02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see, e.g., the point cloud of the third point (at index 2)\n",
    "# (the elevation of the first point should be 0.02)\n",
    "# this is the input for each training and prediction pass\n",
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a permutation of indices (shuffle indices)\n",
    "p = np.random.default_rng().permutation(X.shape[0])\n",
    "\n",
    "# Use the permutated indices to shuffle the array of features\n",
    "X = np.take(X, p, axis=0)\n",
    "\n",
    "# Use the (same) permutated indices to shuffle the array of labels\n",
    "# (It is very important to use the same indices for both features and labels.)\n",
    "y = np.take(y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the PointNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please construct the PointNet model in the following cell. The comments will guide you through the construction. If you want to do it without any guidance, then just delete the comments and start from scratch.\n",
    "\n",
    "**Note:** You can implement the PointNet model either with 1D or 2D convolutional layers, but the explanations that follow are for using 2D convolutional layers. Your suggested task in the end will be to also implement PointNet with the other type of convolutional layers. So, you should do it in both ways to get more familiar with convolutional filters.\n",
    "\n",
    "In order to use a 2D convolutional layer, the input tensor must have 4 dimensions: batch size (B), number of points(N), 1 (in our case to make it 4 dimensional), and channels (C). Using the dimensions in this way, the filter size is (1, 1). If you decide to exchange the 3rd and 4th dimension, and you end up with BxNx3x1, then your filter size is (1,3). Remember that you do not provide the last dimension (the number of channels) in the convolutional filter. Either way, after the first layer, the network will be the same, and the features derived through the convolutional layers will be in the last dimension. Then, (1,1) convolutional filters are used. The dimensions of the input need to be provided to the Input layer.\n",
    "\n",
    "The implementation of the PointNet model follows the architecture presented on the lecture slides. The layers of the MLPs are each composed of a 2D convolutional layer, followed by batch normalization, and a ReLU activation layer. After 5 layers for the first MLP, there is a max pooling layer, and another (second) MLP with 3 layers. In between the 2nd and the 3rd layer (of the second MLP), a dropout layer with a dropout rate of 0.3 can help the training process. The last layer of the second MLP does not have batch normalization or an activation layer. Or rather, a softmax activation layer is used to generate the 9 class scores. \n",
    "\n",
    "Either before or after the softmax activation layer, you need to reduce the dimensions of the tensor from Bx1x1x9 to Bx1x9 by reshaping it to (1,9). Otherwise, the loss function will not work properly.\n",
    "\n",
    "I suggest you use the functional API to define the neural network model as is already given in the following cell. Then, you can add further information from the point cloud like the intensity as an additional input. Your (optional) task later on will be to inject intensity, number of returns, and return number that is also provided by the ISPRS data for the point cloud into the network after the feature extraction (after max pooling). In the functional model, you need to provide the sequence of layers by defining the output of the previous layer to be the input of the current layer.\n",
    "\n",
    "If you have difficulties with the functional API, then you can also use the sequential API. But we have not looked into if you can then inject further information as another input layer into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PointNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_xyz (InputLayer)       [(None, 21, 1, 3)]        0         \n",
      "_________________________________________________________________\n",
      "1_Conv2D (Conv2D)            (None, 21, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "1_BN (BatchNormalization)    (None, 21, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "1_ReLU (Activation)          (None, 21, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "2_Conv2D (Conv2D)            (None, 21, 1, 64)         4160      \n",
      "_________________________________________________________________\n",
      "2_BN (BatchNormalization)    (None, 21, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "2_ReLU (Activation)          (None, 21, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "3_Conv2D (Conv2D)            (None, 21, 1, 64)         4160      \n",
      "_________________________________________________________________\n",
      "3_BN (BatchNormalization)    (None, 21, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "3_ReLU (Activation)          (None, 21, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "4_Conv2D (Conv2D)            (None, 21, 1, 128)        8320      \n",
      "_________________________________________________________________\n",
      "4_BN (BatchNormalization)    (None, 21, 1, 128)        512       \n",
      "_________________________________________________________________\n",
      "4_ReLU (Activation)          (None, 21, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "5_Conv2D (Conv2D)            (None, 21, 1, 1024)       132096    \n",
      "_________________________________________________________________\n",
      "5_BN (BatchNormalization)    (None, 21, 1, 1024)       4096      \n",
      "_________________________________________________________________\n",
      "5_ReLU (Activation)          (None, 21, 1, 1024)       0         \n",
      "_________________________________________________________________\n",
      "MaxPooling (MaxPooling2D)    (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "6_Conv2D (Conv2D)            (None, 1, 1, 512)         524800    \n",
      "_________________________________________________________________\n",
      "6_BN (BatchNormalization)    (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "6_ReLU (Activation)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "7_Conv2D (Conv2D)            (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "7_BN (BatchNormalization)    (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "7_ReLU (Activation)          (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "8_Conv2D (Conv2D)            (None, 1, 1, 9)           2313      \n",
      "_________________________________________________________________\n",
      "softmax_5 (Softmax)          (None, 1, 1, 9)           0         \n",
      "=================================================================\n",
      "Total params: 815,881\n",
      "Trainable params: 811,657\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input layer that takes input of shape Batch size x Number of points x 1 x 3 channel (B, N, 1, 3)\n",
    "input_xyz = keras.layers.Input(shape=(21, 1, 3), dtype='float32', name='Input_xyz')\n",
    "\n",
    "# first layer of first MLP with 64 filters\n",
    "conv1 = keras.layers.Conv2D(filters=64, kernel_size=(1, 1), name ='1_Conv2D')(input_xyz)\n",
    "bn1   = keras.layers.BatchNormalization(name='1_BN')(conv1)\n",
    "relu1 = keras.layers.Activation('relu', name='1_ReLU')(bn1)\n",
    "\n",
    "# second layer of first MLP with 64 filters\n",
    "conv2 = keras.layers.Conv2D(filters=64, kernel_size=(1,1), name=\"2_Conv2D\")(relu1)\n",
    "bn2   = keras.layers.BatchNormalization(name='2_BN')(conv2)\n",
    "relu2 = keras.layers.Activation('relu', name='2_ReLU')(bn2)\n",
    "\n",
    "# third layer of first MLP with 64 filters\n",
    "conv3 = keras.layers.Conv2D(filters=64, kernel_size=(1,1), name=\"3_Conv2D\")(relu2)\n",
    "bn3   = keras.layers.BatchNormalization(name='3_BN')(conv3)\n",
    "relu3 = keras.layers.Activation('relu', name='3_ReLU')(bn3)\n",
    "\n",
    "# fourth layer of first MLP with 128 filters\n",
    "conv4 = keras.layers.Conv2D(filters=128, kernel_size=(1,1), name=\"4_Conv2D\")(relu3)\n",
    "bn4   = keras.layers.BatchNormalization(name='4_BN')(conv4)\n",
    "relu4 = keras.layers.Activation('relu', name='4_ReLU')(bn4)\n",
    "\n",
    "# fifth layer of first MLP with 1024 filters\n",
    "conv5 = keras.layers.Conv2D(filters=1024, kernel_size=(1,1), name=\"5_Conv2D\")(relu4)\n",
    "bn5   = keras.layers.BatchNormalization(name='5_BN')(conv5)\n",
    "relu5 = keras.layers.Activation('relu', name='5_ReLU')(bn5)\n",
    "\n",
    "# max pooling, the pool size is (21, 1)\n",
    "maxpooling = keras.layers.MaxPooling2D(pool_size=(21,1), name=\"MaxPooling\")(relu5)\n",
    "\n",
    "# first layer of the second MLP with 512 filters\n",
    "# (note: the second MLP could also be implemented with dense layers, but then the \n",
    "#        tensor needs to be reshaped to get rid of the extra dimension using (1, 1024).)\n",
    "conv6 = keras.layers.Conv2D(filters=512, kernel_size=(1,1), name=\"6_Conv2D\")(maxpooling)\n",
    "bn6   = keras.layers.BatchNormalization(name='6_BN')(conv6)\n",
    "relu6 = keras.layers.Activation('relu', name='6_ReLU')(bn6)\n",
    "\n",
    "# second layer of the second MLP with 256 filters\n",
    "conv7 = keras.layers.Conv2D(filters=256, kernel_size=(1,1), name=\"7_Conv2D\")(relu6)\n",
    "bn7   = keras.layers.BatchNormalization(name='7_BN')(conv7)\n",
    "relu7 = keras.layers.Activation('relu', name='7_ReLU')(bn7)\n",
    "\n",
    "# insert a dropout layer with rate 0.3\n",
    "dropout = keras.layers.Dropout(0.3)(relu7)\n",
    "\n",
    "# third layer of the second MLP with 9 filters (for classification scores)\n",
    "# WITHOUT batch normalization and without activation function\n",
    "conv8 = keras.layers.Conv2D(filters=9, kernel_size=(1,1), name=\"8_Conv2D\")(dropout)\n",
    "\n",
    "# the tensor needs to be reshaped to (1, 9) to get rid of extra dimension\n",
    "# (If you decided to implement the second MLP with dense layers, then you do not\n",
    "#  need to perform a reshape here as you already did earlier.)\n",
    "#...\n",
    "\n",
    "# softmax actication layer\n",
    "softmax = keras.layers.Softmax()(conv8)\n",
    "\n",
    "# Functional model with defined inputs and outputs\n",
    "model = keras.Model(inputs=[input_xyz], outputs=[softmax], name='PointNet')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The dataset is currently in the shape (753876, 21, 3). But the first dimension of the training data is the number of training instances (753876) and not a dimension that is used for training. The network will randomly use one (or a whole batch) of these 753876 training instances and provide it to the network. Then, the data instance is of shape (21,3). But the network expects a tensor of shape (21,1,3). (The first dimension is the batch size, and we do not need to provide that ourselves.). Therefore, we need to expand the dimensions of the tensor in the second to last (-2) dimension. (We do not actually change the data itself, we just change the definition of the tensor. No data is added and no extra memory is required.)\n",
    "\n",
    "Use maybe 20 (to 40 epochs) for training the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before expand: (753876, 21, 3)\n",
      "Shape after expand:  (753876, 21, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape before expand: {X.shape}\\nShape after expand:  {np.expand_dims(X, axis=-2).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10013/10013 [==============================] - 40s 4ms/step - loss: 0.7975 - accuracy: 0.2128 - val_loss: 0.7534 - val_accuracy: 0.2173\n",
      "Epoch 2/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.7150 - accuracy: 0.2128 - val_loss: 0.7238 - val_accuracy: 0.2131\n",
      "Epoch 3/20\n",
      "10013/10013 [==============================] - 40s 4ms/step - loss: 0.6917 - accuracy: 0.2126 - val_loss: 0.6602 - val_accuracy: 0.2163\n",
      "Epoch 4/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6763 - accuracy: 0.2129 - val_loss: 0.6549 - val_accuracy: 0.2144\n",
      "Epoch 5/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6661 - accuracy: 0.2128 - val_loss: 0.6317 - val_accuracy: 0.2161\n",
      "Epoch 6/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6572 - accuracy: 0.2129 - val_loss: 0.6231 - val_accuracy: 0.2139\n",
      "Epoch 7/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6498 - accuracy: 0.2129 - val_loss: 0.6430 - val_accuracy: 0.2147\n",
      "Epoch 8/20\n",
      "10013/10013 [==============================] - 38s 4ms/step - loss: 0.6432 - accuracy: 0.2131 - val_loss: 0.6325 - val_accuracy: 0.2135\n",
      "Epoch 9/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6380 - accuracy: 0.2130 - val_loss: 0.6122 - val_accuracy: 0.2156\n",
      "Epoch 10/20\n",
      "10013/10013 [==============================] - 38s 4ms/step - loss: 0.6322 - accuracy: 0.2130 - val_loss: 0.6192 - val_accuracy: 0.2120\n",
      "Epoch 11/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6284 - accuracy: 0.2129 - val_loss: 0.6512 - val_accuracy: 0.2091\n",
      "Epoch 12/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6238 - accuracy: 0.2130 - val_loss: 0.6779 - val_accuracy: 0.2078\n",
      "Epoch 13/20\n",
      "10013/10013 [==============================] - 40s 4ms/step - loss: 0.6198 - accuracy: 0.2130 - val_loss: 0.5936 - val_accuracy: 0.2148\n",
      "Epoch 14/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6166 - accuracy: 0.2129 - val_loss: 0.6125 - val_accuracy: 0.2128\n",
      "Epoch 15/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6133 - accuracy: 0.2130 - val_loss: 0.6005 - val_accuracy: 0.2123\n",
      "Epoch 16/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6095 - accuracy: 0.2129 - val_loss: 0.5941 - val_accuracy: 0.2151\n",
      "Epoch 17/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.6078 - accuracy: 0.2128 - val_loss: 0.5881 - val_accuracy: 0.2142\n",
      "Epoch 18/20\n",
      "10013/10013 [==============================] - 38s 4ms/step - loss: 0.6038 - accuracy: 0.2128 - val_loss: 0.5913 - val_accuracy: 0.2137\n",
      "Epoch 19/20\n",
      "10013/10013 [==============================] - 38s 4ms/step - loss: 0.6002 - accuracy: 0.2128 - val_loss: 0.5914 - val_accuracy: 0.2135\n",
      "Epoch 20/20\n",
      "10013/10013 [==============================] - 39s 4ms/step - loss: 0.5985 - accuracy: 0.2127 - val_loss: 0.6052 - val_accuracy: 0.2149\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64  # you can increase this for faster training\n",
    "\n",
    "# train directly on numpy array\n",
    "history = model.fit(np.expand_dims(X, axis=-2), y, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=20, \n",
    "                    validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Vaihingen evaluation data\n",
    "\n",
    "In this exercise, we use the evaluation part of the ISPRS dataset for real testing, so that we get some realistic numbers for quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read \"Vaihingen3D_Evaluation_PointNet_XYZ.npy\" feature matrix of shape (411722, 21, 3)\n",
      "Successfully read \"Vaihingen3D_Evaluation_PointNet_Labels.npy\" label vector of shape  (411722, 1)\n"
     ]
    }
   ],
   "source": [
    "filename_xyz = 'Vaihingen3D_Evaluation_PointNet_XYZ.npy'\n",
    "filename_lab = 'Vaihingen3D_Evaluation_PointNet_Labels.npy'\n",
    "\n",
    "# Load the point clouds with the  x,y,z-values\n",
    "X_test = np.load(os.path.join(root_dir, filename_xyz))\n",
    "print(f'Successfully read \"{filename_xyz}\" feature matrix of shape {X_test.shape}')\n",
    "\n",
    "# Load labels\n",
    "y_test = np.load(os.path.join(root_dir, filename_lab))\n",
    "print(f'Successfully read \"{filename_lab}\" label vector of shape  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset should be around 68%. This low accuracy is to be expected and should be approximately the same as with the hand-craftet features. However, the network now extracts features by itself and we provided less information to it than in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6434/6434 [==============================] - 11s 2ms/step - loss: 0.8912 - accuracy: 0.4069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8911899328231812, 0.40693071484565735]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.expand_dims(X_test, axis=-2), y_test, \n",
    "               batch_size=BATCH_SIZE, \n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, some typical evaluation reports are given.\n",
    "\n",
    "**confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411722, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Powerline</th>\n",
       "      <th>Low vegetation</th>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <th>Car</th>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <th>Roof</th>\n",
       "      <th>Facade</th>\n",
       "      <th>Shrub</th>\n",
       "      <th>Tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Powerline</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low vegetation</th>\n",
       "      <td>0</td>\n",
       "      <td>38191</td>\n",
       "      <td>47279</td>\n",
       "      <td>214</td>\n",
       "      <td>219</td>\n",
       "      <td>838</td>\n",
       "      <td>270</td>\n",
       "      <td>8585</td>\n",
       "      <td>3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <td>0</td>\n",
       "      <td>8662</td>\n",
       "      <td>92788</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "      <td>43</td>\n",
       "      <td>291</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>0</td>\n",
       "      <td>479</td>\n",
       "      <td>31</td>\n",
       "      <td>969</td>\n",
       "      <td>205</td>\n",
       "      <td>198</td>\n",
       "      <td>8</td>\n",
       "      <td>1691</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "      <td>79</td>\n",
       "      <td>67</td>\n",
       "      <td>741</td>\n",
       "      <td>533</td>\n",
       "      <td>199</td>\n",
       "      <td>3610</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roof</th>\n",
       "      <td>3</td>\n",
       "      <td>701</td>\n",
       "      <td>651</td>\n",
       "      <td>45</td>\n",
       "      <td>389</td>\n",
       "      <td>85609</td>\n",
       "      <td>291</td>\n",
       "      <td>2065</td>\n",
       "      <td>19294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facade</th>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>108</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>693</td>\n",
       "      <td>4162</td>\n",
       "      <td>1181</td>\n",
       "      <td>4276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shrub</th>\n",
       "      <td>1</td>\n",
       "      <td>3021</td>\n",
       "      <td>568</td>\n",
       "      <td>187</td>\n",
       "      <td>289</td>\n",
       "      <td>1629</td>\n",
       "      <td>254</td>\n",
       "      <td>12296</td>\n",
       "      <td>6573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>8</td>\n",
       "      <td>890</td>\n",
       "      <td>80</td>\n",
       "      <td>44</td>\n",
       "      <td>94</td>\n",
       "      <td>5291</td>\n",
       "      <td>535</td>\n",
       "      <td>5191</td>\n",
       "      <td>42093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Powerline  Low vegetation  Impervious surfaces  Car  \\\n",
       "Powerline                  100               1                    2    0   \n",
       "Low vegetation               0           38191                47279  214   \n",
       "Impervious surfaces          0            8662                92788   14   \n",
       "Car                          0             479                   31  969   \n",
       "Fence/Hedge                  0             886                   79   67   \n",
       "Roof                         3             701                  651   45   \n",
       "Facade                       7             742                  108   24   \n",
       "Shrub                        1            3021                  568  187   \n",
       "Tree                         8             890                   80   44   \n",
       "\n",
       "                     Fence/Hedge   Roof  Facade  Shrub   Tree  \n",
       "Powerline                      0     98      17      6    376  \n",
       "Low vegetation               219    838     270   8585   3094  \n",
       "Impervious surfaces            8    120      43    291     60  \n",
       "Car                          205    198       8   1691    127  \n",
       "Fence/Hedge                  741    533     199   3610   1307  \n",
       "Roof                         389  85609     291   2065  19294  \n",
       "Facade                        31    693    4162   1181   4276  \n",
       "Shrub                        289   1629     254  12296   6573  \n",
       "Tree                          94   5291     535   5191  42093  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "class_names = ['Powerline', 'Low vegetation', 'Impervious surfaces', 'Car', 'Fence/Hedge', 'Roof', 'Facade', 'Shrub', 'Tree']\n",
    "\n",
    "y_test_pred = np.argmax(model.predict(np.expand_dims(X_test, axis=-2)), axis=-1)[:,:,0]\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "df = pd.DataFrame(data=cm, columns=class_names, index=class_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision, recall, F1-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall   : {recall:.2f}')\n",
    "print(f'F1       : {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save colorized point cloud of testing data\n",
    "\n",
    "For outputting the point cloud with the predicted class labels, we need the original 3D point cloud with original x,y,z-coordinates. The points are in the same order as the NumPy arrays that provide the x,y,z-coordinates of the small point clouds (with the 21-points) and the labels. (Remember that the points in the small point clouds are all centered by the first point, so that we do not have the original coordinates anymore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pc = 'Vaihingen3D_Evaluation.pts'\n",
    "\n",
    "xyz_df = pd.read_csv(os.path.join(root_dir, filename_pc), \n",
    "                     sep=\" \", \n",
    "                     names=['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns', 'label'],\n",
    "                     usecols=['x', 'y', 'z'],\n",
    "                     header=None)\n",
    "\n",
    "print(f'Successfully read \"{filename_pc}\" of shape {xyz_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_colorized_point_cloud(xyz_df.to_numpy(), y_test_pred, 'Vaihingen3D_Evaluation_Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we used PointNet as a feature encoder for points with their 20-point neighborhood for the classification of 3D point clouds. The feature encoding is the part until (including) max pooling. What follows is a typical classification network with fully connected layers that are implemented with convolutional filters and with softmax . (A convolutional filter that has the same size as the input has the same effect as a fully connected layer. Only the interpretation of the data is different with regard to tensor dimensions.)\n",
    "\n",
    "As seen by the results, the quality is approximately the same as with hand-crafted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Below are a few ideas for tasks that could still be done for PointNet in order to practice more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Predictions on large dataset\n",
    "\n",
    "Use the large dataset for prediction and outputting the colorized point cloud. The files are called \"Vaihingen3D_Large_PointNet_XYZ.npy\" for the small input point clouds. And the points with the original x,y,z-coordinates are stored in \"Vaihingen3D_Large_Points.csv.gz\". As there are no labels in this large dataset, it can only be used for prediction and not for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 1D Convolutions\n",
    "\n",
    "Implement the PointNet model also with 1D convolutions. Be careful with the tensor dimensions (input data, input and filter sizes, reshape?, etc.). When working with 1D convolutions, your whole model needs to process data with 3 dimensions: batch size, number of points, channels. (You do not need to provide batch sizes, so the definition of layers is without the batch size dimension.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Intensity, return number, number of returns\n",
    "\n",
    "You can include the 3 other columns of the dataset, 'intensity', 'return_number', and 'number_of_returns', as further (second) input to the network and inject this data into the network after the max pooling layer. The following cells should give you some information on how to implement this.\n",
    "\n",
    "Explanation on the sensor features:\n",
    "- Intensity is the intensity with which the laser beam was reflected. Flat, impervious surfaces typically have a higher intensity as vegetation.\n",
    "- The laser beam is sometimes reflected several times, e.g. going through a tree, the laser beam is reflected at the branches, and several returns are registered and digitized. Number of returns is the total number of these reflections.\n",
    "- The laser beam is typically not going straight downwards, so there is a tilt in its direction. The different returns (of the same beam) at different elevation levels, lead to 3D points with different x,y-coordinates. Therefore, all points have individual coordinates and the return number denotes the \"index\" of the return it was derived from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the point cloud data with intensity, return number, number of returns. Do not forget to also shuffle it with the same permutation as the xyz and labels. But make absolutely sure that the shape of the tensor does not change. Check with the .shape() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pc_df = pd.read_csv(os.path.join(root_dir, 'Vaihingen3D_Training.pts'), \n",
    "                     sep=\" \", \n",
    "                     names=['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns', 'label'],\n",
    "                     header=None)\n",
    "\n",
    "intensity = pc_df[['intensity', 'return_number', 'number_of_returns']].to_numpy(dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a second input layer (right after the first input layer) with input dimensions (3). (Depending on your implementation of PointNet, you will need to reshape the tensor within the nextwork before the concatenation with the extracted features.) In the summary, your input layer will only appear if it is actually used and at the position where it is used. So please be aware that you might not find it right away or at a position you might not have expected it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_intensity = keras.layers.Input(shape=(3), dtype='float32', name='Input_intensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a concatenation layer to concatenate the (1024) channels of the extracted features and with the (3) channels of the intensity input. But before that, you have to adjust the dimensions of the second input, so that it fits the global features.\n",
    "\n",
    "If your output dimensions from the max pooling layer are (None, 1, 1, 1024) and the output dimensions of your intensity are (None, 1, 3), then reshape your intensity to (None, 1, 1, 3).\n",
    "\n",
    "Then concatenate the output of the pooling layer with the reshape layer of the intensity. Do not forget to change the next layer to take the new concat output as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_intensity = keras.layers.Reshape((1, 1, 3), name='ReshapeIntensity')(input_intensity)\n",
    "\n",
    "concat = keras.layers.Concatenate(name='Concatenate')([pool,reshape_intensity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model must be defined to have two input layers. Just exchange the respective line in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_xyz, input_intensity], outputs=[softmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the model, you have to provide a tuple (a pair) of the inputs, by using the parenthesis (for the tuple) and the two input Numpy arrays. (You could also expand the dimension of the intensity input before it is inputted it into the fit() function instead of reshaping it in the model. The effect, however, is the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit((np.expand_dims(X, axis=-2), intensity), y, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that you also need to provide a tuple as input for the predictions you make. The evaluation dataset for testing is called \"Vaihingen3D_Evaluation.pts\", from which you need to extract the intensity, etc. for the predictions.\n",
    "\n",
    "Your accuracy on the evaluation dataset should get up about 8% to 76%. As you can see, this kind of sensor information can be to quite an improvement. The added value will, however, dimish once we continue with multi-scale feature extraction.\n",
    "\n",
    "But also output the colorized point cloud and check what really changed. You can also take the labeled file \"Vaihingen3D_Training.pts\", extract the x,y,z-coordinates, and the labels as two numpy arrays, and save it with the helper function as a colorized point cloud as a referenc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
