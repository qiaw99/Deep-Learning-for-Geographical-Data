{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - Feature-Based Classification of 3D Point Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we use a multi-layer perceptron (fully connected feedforward neural network) to classify individual points from a 3D point cloud by a set of 12 pre-computed features. The inputs of the network are the 12 feature values of a point, and it predicts the label for this point only.\n",
    "\n",
    "The dataset used is the one for the 3D semantic labeling contest of the ISPRS. You can find more information about the dataset on the web site of the ISPRS (https://www2.isprs.org/commissions/comm2/wg4/benchmark/3d-semantic-labeling/). It was generated for aerial 3D point cloud classification before the time of Deep Learning, and is really too small for Deep Learning to sufficiently learn from it. But it is open and should be sufficient to learn about the concepts and methods. With the Deep Learning approach presented here, you should get typical accuracies as with pre-Deep Learning machine learning methods.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Repeat fully connected neural networks\n",
    "- Perform and understand a feature-based classification using neural networks\n",
    "\n",
    "**Please note:** In order to get more out of the dataset and this methodology, we will cheat a little (ok, not a little, but a lot) by using both the training and testing (validation) part of the benchmark dataset. And only use random points within the dataset for validating our training process. (Validating in the sense to make sure it actually works.) Normally, we should have seperate regions for training, validation, and testing. But the dataset does not provide enough variety for this kind of Deep Learning approach. In future exercises, when we fully make use of Deep Learning as a feature encoder, we will use the dataset as intended and even get better results than presented here.\n",
    "\n",
    "**Your tasks:**\n",
    "- Go through and understand the implementation as well as the underlying methodology\n",
    "- Define and implemenent the neural network model\n",
    "- Experiment with hyperparameters\n",
    "- Compare the results of the small training dataset with the results of the large dataset\n",
    "\n",
    "(The implementation of how the points are colorized in the respective helper function uses Numpy vector operations. This might not be very obvious to understand without any further experience or explanation. I suggest you skip over this function and just accept that it colorizes and saves point clouds. The neural network part is more important.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:4 GPU\n",
      "\n",
      "Keras version: 2.4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# Import Keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the installed Keras version\n",
    "print(f'\\nKeras version: {keras.__version__}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Numpy and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.18.5\n",
      "Pandas version: 1.0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f'Numpy version:  {np.__version__}\\nPandas version: {pd.__version__}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function **load_csv_file()** reads a file in csv (comma seperated values) format, outputs its shape and columns, and returns the data as a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def load_csv_file(filename):\n",
    "\n",
    "    root_dir = str(Path.home()) + r'/coursematerial/GIS/ISPRS/PointsWithFeatures'\n",
    "\n",
    "    df = pd.read_csv(os.path.join(root_dir, filename), sep=\" \")\n",
    "    \n",
    "    print(f'Loaded \"{filename}\"\\n  Shape: {df.shape}')\n",
    "\n",
    "    print('  Columns:', ', '.join([c for c in df.columns]), '\\n')\n",
    "    \n",
    "    return df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function **save_colorized_point_cloud()** assigns each point of the Numpy array xyz a color according to the labels y, and saves the colorized points as a csv file with the given filename. The color coding is the same as used for the ISPRS benchmark.\n",
    "\n",
    "You can then download this file on your computer, and visualize it with the open source software CloudCompare (https://www.danielgm.net/cc/). CloudCompare is probably the most used (open source) point cloud visualization software, especially in the field of geodata. When you open the file in CloudCompare, make sure you change the file filter to either all or csv. (Sorry about making you install a software on your own computer. We normally have this software on the computers in the computer pool. Depending on your hardware, the visualization of the large point cloud might be slow.) For the two dialogs that pop up, you can press \"Yes to All\". It translates your data into the estimated centroid of the dataset and interprets the columns of the data to determine what column is x, y, z, red, green, and blue.\n",
    "\n",
    "If you click your dataset in the list view of CloudCompare that is called 'DB Tree', and then move your mouse in the 3D view to the top-left corner, then you can increase the points size, which helps when you zoom into the point clouds. You can do this also in the 'Properties' below 'DB Tree'. Try both ways and decide, which you find more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_colorized_point_cloud(xyz, y, filename):\n",
    "\n",
    "    color_map = np.array([\n",
    "        [255, 255, 125],\n",
    "        [  0, 255, 255],\n",
    "        [255, 255, 255],\n",
    "        [255, 255,   0],\n",
    "        [  0, 255, 125],\n",
    "        [  0,   0, 255],\n",
    "        [  0, 125, 255],\n",
    "        [125, 255,   0],\n",
    "        [  0, 255,   0]])\n",
    "    \n",
    "    u, inverses = np.unique(y, return_inverse=True)    \n",
    "    \n",
    "    colors = color_map[inverses]\n",
    "    \n",
    "    df = pd.DataFrame(xyz, columns=['x', 'y', 'z'])    \n",
    "\n",
    "    df['red'] = pd.Series(data=colors[:,0], name='red')\n",
    "    df['green'] = pd.Series(data=colors[:,1], name='green')\n",
    "    df['blue'] = pd.Series(data=colors[:,2], name='blue')\n",
    "    \n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    print(f'Saved \"{filename}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "Load the features and the labels as Numpy arrays from the files provided in the course material folder. The features are 12 selected features mostly from the lecture that proved to be effective for point cloud classification and on this particular data set. For other data sets, other features might work better and should be selected accordingly.\n",
    "\n",
    "The only feature that is not mentioned in the lecture is 'ground_z', which is the height of the point over the digital elevation model. This is, of course, a very strong feature, and it helps to differentiate between impervious surfaces and roofs. The digital elevation model was generated from a prior classification that differentiates between ground and non-ground points. We therefore introduce already some strong prior information with this feature. However, the generation of digital terrain models is a long solved problem and such data exists from many sources. (There was a bug in the data preparation, and the 'ground_z' values are negated. But this will not effect the classification in any way. The neural network does not know that we do not live in an upside down world.)\n",
    "\n",
    "The 2D geometric features from a discretized grid were generated with a bin size of 1m by 1m.\n",
    "\n",
    "As k for the nearest neighbor search, a value of 21 was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded \"Vaihingen3D_FEX12.csv.gz\"\n",
      "  Shape: (1165598, 12)\n",
      "  Columns: planarity, scattering, omnivariance, sum_eigenvalues, change_of_curvature, verticality, delta_z_knn, std_z_knn, delta_z, std_z, eigenvalue 3, ground_z \n",
      "\n",
      "Loaded \"Vaihingen3D_Labels.csv.gz\"\n",
      "  Shape: (1165598, 1)\n",
      "  Columns: label \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numpy array with pre-calculated features\n",
    "X = load_csv_file('Vaihingen3D_FEX12.csv.gz')\n",
    "\n",
    "# Numpy array with labels\n",
    "y = load_csv_file('Vaihingen3D_Labels.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class names of the labels, and determine and output the number of points per class. You will notice that some classes (e.g. powerline, car) are strongly underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Powerline           : 1146\n",
      "Low vegetation      : 279540\n",
      "Impervious surfaces : 295709\n",
      "Car                 : 8322\n",
      "Fence/Hedge         : 19492\n",
      "Roof                : 261093\n",
      "Facade              : 38474\n",
      "Shrub               : 72423\n",
      "Tree                : 189399\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Powerline', 'Low vegetation', 'Impervious surfaces', 'Car', 'Fence/Hedge', 'Roof', 'Facade', 'Shrub', 'Tree']\n",
    "u, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "for i in u:\n",
    "    print(f'{class_names[i]:20}: {counts[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could normalize the features, but it has no noticeable effect. But you can try it out by uncommenting and executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = tf.keras.utils.normalize(X, axis=-1, order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the training data\n",
    "\n",
    "The data set used in this exercise is a toy dataset and not sufficiently large for Deep Learning. We therefore cheat a little in what part of the data we use for training and validation. And we do not evaluate the performance on test data. Rather, we use as much data for training and just enough for validation to observe how the training process is doing. In the end, we do predictions on a larger data set (where we have no labels) and do a visual evaluation (testing) instead.\n",
    "\n",
    "In the following cell, the input point features and the labels are shuffled (using a permutation of indices). The reason is that the points of the input 3D point cloud are ordered according to how they were acquired by the laser scanner. Therefore, the points are close to each other. If you let TensorFlow reserve a certain percentage of this data as validation data, then it takes this validation part from the end of the data set. Unfortunately, this validation data is then a region of the data set that is not represented well in the other part, the training data. So, validation data and training data would not be much alike. This would make the evaluation scores rather low. Also, we would loose that underrepresented part of the data for training, which might lower the quality of the results for the large data set.\n",
    "\n",
    "Therefore, another approach is taken. Since we classify each point individually, we just shuffle all the points and take a certain percentage of the shuffled points as validation data. These validation points are now better distributed in the data set. Of course, the validation points are also located close to the points that have been used for training. The evaluation is therefore not really fair and has little validity for unseen data. It just states how well the model would work on points from objects that have similar features as the ones in the training data. But like already stated, the data set is just not large enough for this task.\n",
    "\n",
    "You can try to not shuffle the data and check out how it effects the training.\n",
    "\n",
    "The data set is also a benchmark data set for (non Deep Learning) semantic segmentation. (However, the original training data and evaluation data is now concatenated in one large data set.) If you keep the sequential order, you could use 40% of the data for training, 20% for validation, and 40% for testing. That should be approximately be the original setup and see how this network performs.\n",
    "\n",
    "Once we continue with the more elaborate Deep Learning architectures for 3D point cloud classification in future exercises, we will use the benchmark data as it was supposed to and will be more fair towards the evaluation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a permutation of indices (shuffle indices)\n",
    "p = np.random.default_rng().permutation(X.shape[0])\n",
    "\n",
    "# Use the permutated indices to shuffle the array of features\n",
    "X = np.take(X, p, axis=0)\n",
    "\n",
    "# Use the (same) permutated indices to shuffle the array of labels\n",
    "# (It is very important to use the same indices for both features and labels.)\n",
    "y = np.take(y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "Define here your classification model (with the sequential API) as you have learned in previous exerices. Use only dense layers. Convolutions would not make sense on the features as they are not in any way (spatially or otherwise) related. ReLU activation and Xavier (He normal) initialization should work fine, but you can also try out different ones. Check out if batch normalization or Dropout makes any difference.\n",
    "\n",
    "Most relevant will probably be the number of layers and the number of neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(12,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer=keras.initializers.HeNormal()),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_initializer=keras.initializers.HeNormal()),\n",
    "    tf.keras.layers.Dense(9, activation=\"softmax\", kernel_initializer=keras.initializers.HeNormal())],\n",
    "    name='point_cloud_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() method outputs a summary of the model, including the name of each layer, its type, the output shape, and the number of parameters. The first dimension (row) of the output shape determines the batch size, where None means that the batch size can be anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"point_cloud_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 37,513\n",
      "Trainable params: 37,257\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with loss function, optimizer, and metrics to be calculated.\n",
    "\n",
    "The sparse categorical crossentropy is used as each instance of the training data has a target class index (with a value between 0 and 9) as its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model with the fit method, prodiving the features (X) as the training data and the labels y. The percentage of validation to be used is defined in the validation_split argument. Training for 5 epochs should get quite good results that might increase with further epochs. Just note that since we have no real seperate validation and testing data, you will not notice any overfitting. You might just see it visually in the predictions of the larger data, when the quality of class predictions decreases for the whole area, but is very good for the training area (that is included in the larger dataset).\n",
    "\n",
    "The batch size really improves training time and model convergence. (When you do not provide any batch size yourself, a batch size of 32 is the default. This will slow down your training considerably. Try it out!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7285/7285 [==============================] - 13s 2ms/step - loss: 0.7474 - accuracy: 0.7015 - val_loss: 0.6976 - val_accuracy: 0.7173\n",
      "Epoch 2/5\n",
      "7285/7285 [==============================] - 13s 2ms/step - loss: 0.7051 - accuracy: 0.7144 - val_loss: 0.6897 - val_accuracy: 0.7184\n",
      "Epoch 3/5\n",
      "7285/7285 [==============================] - 13s 2ms/step - loss: 0.6944 - accuracy: 0.7175 - val_loss: 0.6819 - val_accuracy: 0.7220\n",
      "Epoch 4/5\n",
      "7285/7285 [==============================] - 13s 2ms/step - loss: 0.6872 - accuracy: 0.7192 - val_loss: 0.6773 - val_accuracy: 0.7224\n",
      "Epoch 5/5\n",
      "7285/7285 [==============================] - 13s 2ms/step - loss: 0.6817 - accuracy: 0.7208 - val_loss: 0.6740 - val_accuracy: 0.7244\n"
     ]
    }
   ],
   "source": [
    "# Provide the training data as a Numpy array X.\n",
    "history = model.fit(x=X, y=y, batch_size=128, epochs=5, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your validation accuracy should be around 72%, maybe even a little higher. But this is the best that can be achieved in a feature-based approach on this particular dataset. (Beware of overfitting.)\n",
    "\n",
    "If you did not shuffle the data, then your accuracy should be a little lower. But it also depends on the percentage of the validation split in the fit method. If only 10% is used, then the validation data probably contains mostly ground points and vegetation that are quite easy to differentiate. (This is because without shuffling, it cuts the last 10% of points, which is the border of the region without any buildings.) And you should get almost the same accuracy. But if you use 20% as validation data, then it includes more buildings that have different properties as the ones in the other 80%. Then your accuracy drops to maybe 67%.\n",
    "\n",
    "With no test data, we unfortunately cannot determine any performance measures, so we skip this part and note that our validation accuracy is not really a fair measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for training data\n",
    "\n",
    "Again, this is not a fair evaluation of the model. But the dataset is small enough so that it can be visualized even on a slow notebook. The following cell predicts the labels for the training data, uses the predicted labels to colorize the point cloud, and saves the output as a csv file. (You can then visualize it in CloudCompare (as noted above)).\n",
    "\n",
    "Because the training data contains no coordinates, we also need load the original points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36425/36425 [==============================] - 19s 529us/step\n",
      "Loaded \"Vaihingen3D_Points.csv.gz\"\n",
      "  Shape: (1165598, 3)\n",
      "  Columns: x, y, z \n",
      "\n",
      "Saved \"Vaihingen3D_Results.csv\"\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X, verbose=1), axis=-1)\n",
    "\n",
    "xyz = load_csv_file('Vaihingen3D_Points.csv.gz')\n",
    "\n",
    "# make sure to shuffle also the xyz in the same way\n",
    "xyz = np.take(xyz, p, axis=0)\n",
    "\n",
    "save_colorized_point_cloud(xyz, y_pred, 'Vaihingen3D_Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated evaluation values that follow are in no way legitimate (as we did the predictions on the training data) and should never be performed like this in practice.\n",
    "\n",
    "It is just to show the implementation of how to get the confusion matrix, the precision, recall, and f1 score for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Powerline</th>\n",
       "      <th>Low vegetation</th>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <th>Car</th>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <th>Roof</th>\n",
       "      <th>Facade</th>\n",
       "      <th>Shrub</th>\n",
       "      <th>Tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Powerline</th>\n",
       "      <td>199</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low vegetation</th>\n",
       "      <td>0</td>\n",
       "      <td>142112</td>\n",
       "      <td>118376</td>\n",
       "      <td>230</td>\n",
       "      <td>377</td>\n",
       "      <td>3069</td>\n",
       "      <td>646</td>\n",
       "      <td>12072</td>\n",
       "      <td>2658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Impervious surfaces</th>\n",
       "      <td>0</td>\n",
       "      <td>46239</td>\n",
       "      <td>247994</td>\n",
       "      <td>68</td>\n",
       "      <td>84</td>\n",
       "      <td>229</td>\n",
       "      <td>159</td>\n",
       "      <td>847</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>0</td>\n",
       "      <td>1758</td>\n",
       "      <td>18</td>\n",
       "      <td>816</td>\n",
       "      <td>416</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>4804</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence/Hedge</th>\n",
       "      <td>0</td>\n",
       "      <td>2930</td>\n",
       "      <td>39</td>\n",
       "      <td>367</td>\n",
       "      <td>1409</td>\n",
       "      <td>1439</td>\n",
       "      <td>151</td>\n",
       "      <td>11490</td>\n",
       "      <td>1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roof</th>\n",
       "      <td>5</td>\n",
       "      <td>1128</td>\n",
       "      <td>958</td>\n",
       "      <td>38</td>\n",
       "      <td>241</td>\n",
       "      <td>234085</td>\n",
       "      <td>1247</td>\n",
       "      <td>3453</td>\n",
       "      <td>19938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facade</th>\n",
       "      <td>45</td>\n",
       "      <td>2079</td>\n",
       "      <td>153</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>4115</td>\n",
       "      <td>18113</td>\n",
       "      <td>2741</td>\n",
       "      <td>11169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shrub</th>\n",
       "      <td>10</td>\n",
       "      <td>13158</td>\n",
       "      <td>919</td>\n",
       "      <td>384</td>\n",
       "      <td>645</td>\n",
       "      <td>2398</td>\n",
       "      <td>1077</td>\n",
       "      <td>40963</td>\n",
       "      <td>12869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>29</td>\n",
       "      <td>3482</td>\n",
       "      <td>167</td>\n",
       "      <td>82</td>\n",
       "      <td>193</td>\n",
       "      <td>7749</td>\n",
       "      <td>2184</td>\n",
       "      <td>16536</td>\n",
       "      <td>158977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Powerline  Low vegetation  Impervious surfaces  Car  \\\n",
       "Powerline                  199               6                   33    0   \n",
       "Low vegetation               0          142112               118376  230   \n",
       "Impervious surfaces          0           46239               247994   68   \n",
       "Car                          0            1758                   18  816   \n",
       "Fence/Hedge                  0            2930                   39  367   \n",
       "Roof                         5            1128                  958   38   \n",
       "Facade                      45            2079                  153   32   \n",
       "Shrub                       10           13158                  919  384   \n",
       "Tree                        29            3482                  167   82   \n",
       "\n",
       "                     Fence/Hedge    Roof  Facade  Shrub    Tree  \n",
       "Powerline                      0      79      33     10     786  \n",
       "Low vegetation               377    3069     646  12072    2658  \n",
       "Impervious surfaces           84     229     159    847      89  \n",
       "Car                          416     426      18   4804      66  \n",
       "Fence/Hedge                 1409    1439     151  11490    1667  \n",
       "Roof                         241  234085    1247   3453   19938  \n",
       "Facade                        27    4115   18113   2741   11169  \n",
       "Shrub                        645    2398    1077  40963   12869  \n",
       "Tree                         193    7749    2184  16536  158977  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "df = pd.DataFrame(data=cm, columns=class_names, index=class_names)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.72\n",
      "Recall   : 0.72\n",
      "F1       : 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y, y_pred, average='weighted')\n",
    "recall = recall_score(y, y_pred, average='weighted')\n",
    "f1 = f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall   : {recall:.2f}')\n",
    "print(f'F1       : {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on large 3D point cloud\n",
    "\n",
    "In the following, we do the predictions on a larger area around and including the ISPRS benchmark area, and save the result as a colorized point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded \"Vaihingen3D_Large_FEX12.csv.gz\"\n",
      "  Shape: (6112862, 12)\n",
      "  Columns: planarity, scattering, omnivariance, sum_eigenvalues, change_of_curvature, verticality, delta_z_knn, std_z_knn, delta_z, std_z, eigenvalue 3, ground_z \n",
      "\n",
      "191027/191027 [==============================] - 99s 520us/step\n",
      "Loaded \"Vaihingen3D_Large_Points.csv.gz\"\n",
      "  Shape: (6112862, 3)\n",
      "  Columns: x, y, z \n",
      "\n",
      "Saved \"Vaihingen3D_Large_Results.csv\"\n"
     ]
    }
   ],
   "source": [
    "X_large = load_csv_file('Vaihingen3D_Large_FEX12.csv.gz')\n",
    "\n",
    "y_large_pred = np.argmax(model.predict(X_large, verbose=1), axis=-1)\n",
    "\n",
    "xyz_large = load_csv_file('Vaihingen3D_Large_Points.csv.gz')\n",
    "\n",
    "save_colorized_point_cloud(xyz_large, y_large_pred, 'Vaihingen3D_Large_Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual evaluation\n",
    "\n",
    "Use CloudCompare to visually evaluate the results. You can load in both datasets, the small one from the training data and the larger one. By clicking \"Yes to All\", they should be prefectly aligned with one another. Then,  you can switch them on and off in the \"DB Tree\" and compare how the model performed on the larger dataset in comparison to the training data. Buildings that are similar to the ones in the training data should be rather well captured, while other might be worse.\n",
    "\n",
    "While preparing this notebook, we noticed that the prediction on the larger dataset does not give the same results for the points that are also included in the training area. This is rather strange and should not happen. The reasons could be a bug or the use of different parameters when preparing the data. We cannot exclude this as a source of error. More likely, however, is that different digital terrain models were generated for the small dataset and the larger dataset, and that the elevations differ, e.g., because of the used interpolation and ground/non-ground point classification. As we will not continue with the dataset of features, we decided not to follow up on this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "In the next lecture and exercise, we will have the Deep Learning model calculate relevant features by itself. This will reflect better on the recent advances of neural networks as feature encoders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
