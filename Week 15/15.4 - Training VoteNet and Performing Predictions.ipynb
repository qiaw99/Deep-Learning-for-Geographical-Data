{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.4 - Training VoteNet and Performing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Like many other neural network implementations, VoteNet provides a script to train the network. However, this script has to be modified, so that it is executable under the environment of the server being used, and so that it can also take the ISPRS data as input.\n",
    "\n",
    "Open the \"train.py\" file within the votenet main folder.\n",
    "\n",
    "Below the comment \"# Create Dataset and Dataloader\", you can find two sections which, depending on the call arguments of the script, create a dataset object of the respective dataset class for either the Sun RGB or ScanNet data. You can take the section of ScanNet as a template (starting with \"elif FLAGS.dataset == 'scannet':\"), **copy and paste it right before the \"else:\" statement, and adapt it for the ISPRS data**. You need to exchange the argument dataset flag with a string you want to use when calling the Python train script (e.g. \"isprs\"), provide the name of the folder where the dataset class and dataset config class are located (\"ISPRS\"), and the names of the dataset (ISPRSDetectionVotesDataset) and dataset configuration (ISPRSDatasetConfig) classes. For the parameter use_color, put the Boolean value False as no color information is provided by the ISPRS dataset.\n",
    "\n",
    "To have a check on the correctness of the code, we provide the corresponding code in the code cell below. But better to do the changes by hand instead of just using copy and paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif FLAGS.dataset == 'isprs':\n",
    "    sys.path.append(os.path.join(ROOT_DIR, 'ISPRS'))\n",
    "    from ISPRS_detection_dataset import ISPRSDetectionVotesDataset, MAX_NUM_OBJ\n",
    "    from model_util_ISPRS import ISPRSDatasetConfig\n",
    "    DATASET_CONFIG = ISPRSDatasetConfig()\n",
    "    TRAIN_DATASET = ISPRSDetectionVotesDataset('train', num_points=NUM_POINT,\n",
    "        augment=False,\n",
    "        use_color=False, use_height=(not FLAGS.no_height))\n",
    "    TEST_DATASET = ISPRSDetectionVotesDataset('val', num_points=NUM_POINT,\n",
    "        augment=False,\n",
    "        use_color=False, use_height=(not FLAGS.no_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally these would be the only necessary changes to make VoteNet capable to also use the ISPRS dataset classes. However, the VoteNet implementation also uses Tensorflow's Tensorboard visualizer (although the implementation of VoteNet is actually based on PyTorch). And the visualizer is not usable on the server in this form. Therefore, all the references of the visualizer need to be commented out.\n",
    "\n",
    "**Find all lines of code that uses \"TfVisualizer\" and the variables named \"TRAIN_VISUALIZER\" and \"TEST_VISUALIZER\" and comment them out (by placing the #-symbol at the beginning of the line)** There should be 7 lines of code that need to be commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you close the \"train.py\" file, take a look at the parameters that VoteNet accepts, read their descriptions, and check their default values. \n",
    "\n",
    "Alternatively, start the train.py script with the -h (help) argument to get the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--model MODEL] [--dataset DATASET]\n",
      "                [--checkpoint_path CHECKPOINT_PATH] [--log_dir LOG_DIR]\n",
      "                [--dump_dir DUMP_DIR] [--num_point NUM_POINT]\n",
      "                [--num_target NUM_TARGET] [--vote_factor VOTE_FACTOR]\n",
      "                [--cluster_sampling CLUSTER_SAMPLING]\n",
      "                [--ap_iou_thresh AP_IOU_THRESH] [--max_epoch MAX_EPOCH]\n",
      "                [--batch_size BATCH_SIZE] [--learning_rate LEARNING_RATE]\n",
      "                [--weight_decay WEIGHT_DECAY] [--bn_decay_step BN_DECAY_STEP]\n",
      "                [--bn_decay_rate BN_DECAY_RATE]\n",
      "                [--lr_decay_steps LR_DECAY_STEPS]\n",
      "                [--lr_decay_rates LR_DECAY_RATES] [--no_height] [--use_color]\n",
      "                [--use_sunrgbd_v2] [--overwrite] [--dump_results]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         Model file name [default: votenet]\n",
      "  --dataset DATASET     Dataset name. sunrgbd or scannet. [default: sunrgbd]\n",
      "  --checkpoint_path CHECKPOINT_PATH\n",
      "                        Model checkpoint path [default: None]\n",
      "  --log_dir LOG_DIR     Dump dir to save model checkpoint [default: log]\n",
      "  --dump_dir DUMP_DIR   Dump dir to save sample outputs [default: None]\n",
      "  --num_point NUM_POINT\n",
      "                        Point Number [default: 20000]\n",
      "  --num_target NUM_TARGET\n",
      "                        Proposal number [default: 256]\n",
      "  --vote_factor VOTE_FACTOR\n",
      "                        Vote factor [default: 1]\n",
      "  --cluster_sampling CLUSTER_SAMPLING\n",
      "                        Sampling strategy for vote clusters: vote_fps,\n",
      "                        seed_fps, random [default: vote_fps]\n",
      "  --ap_iou_thresh AP_IOU_THRESH\n",
      "                        AP IoU threshold [default: 0.25]\n",
      "  --max_epoch MAX_EPOCH\n",
      "                        Epoch to run [default: 180]\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch Size during training [default: 8]\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        Initial learning rate [default: 0.001]\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Optimization L2 weight decay [default: 0]\n",
      "  --bn_decay_step BN_DECAY_STEP\n",
      "                        Period of BN decay (in epochs) [default: 20]\n",
      "  --bn_decay_rate BN_DECAY_RATE\n",
      "                        Decay rate for BN decay [default: 0.5]\n",
      "  --lr_decay_steps LR_DECAY_STEPS\n",
      "                        When to decay the learning rate (in epochs) [default:\n",
      "                        80,120,160]\n",
      "  --lr_decay_rates LR_DECAY_RATES\n",
      "                        Decay rates for lr decay [default: 0.1,0.1,0.1]\n",
      "  --no_height           Do NOT use height signal in input.\n",
      "  --use_color           Use RGB color in input.\n",
      "  --use_sunrgbd_v2      Use V2 box labels for SUN RGB-D dataset\n",
      "  --overwrite           Overwrite existing log and dump folders.\n",
      "  --dump_results        Dump results.\n"
     ]
    }
   ],
   "source": [
    "!python votenet/train.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network uses by default the Sun RGB dataset, 20.000 input points per patch, trains for 180 epochs with a batch size of 8.\n",
    "\n",
    "To train VoteNet with the ISPRS data, better open a terminal, change the current directory to the \"votenet\" directory, and run the train.py script with the following arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 python train.py --dataset isprs  --log_dir log_isprs --num_point 100000 --max_epoch 2 --batch_size 1 --lr_decay_steps '160, 240, 320'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of epochs is set to 2, which is not enough to effectively train the network. You could run it for more epochs, but it would take a few hours/days to get good results. From our experience, 400 epochs with the provided dataset gives quite good results. (Although we only tested it on the training data itself, which does not allow a conclusive statement for unseen data yet!)\n",
    "\n",
    "**The following arguments are the more important once for running the train script:**\n",
    "\n",
    "- The preceding \"CUDA_VISIBLE_DEVICES=\" allows you to define which GPU(s) the training process sees. Please change the GPU id from 7 to a GPU that is not occupied. Use \"nvidia-smi\" on the terminal to check how the GPUs are currently occupied.\n",
    "\n",
    "- The argument --log_dir specifies in which directory (here log_isprs) the checkpoint and the log is stored. The checkpoint contains all the weights of the network, and can be used to continue training the network, and for evaluation and for prediction. (It is basically a saved state of the network for future use.)\n",
    "\n",
    "- The provided patches have 100.000 points, which are defined by --num_points.\n",
    "\n",
    "- The batch size is 1, because of memory limitations on the server side itself, and not the GPU.\n",
    "\n",
    "- A number of learning rate decay steps (--lr_decay_steps) are specified that happen at epoch 160, 240, and 320. The learning decay rate is by default 0.1 for all 3 steps. What this means is that at each of these 3 steps, the new learning rate is reduced to 10% of the old learning rate (lr = lr * 0.1).\n",
    "\n",
    "**Final notes on training:**\n",
    "- Unfortunately, the server instance on the provided server will stop after a while when the connection with the client is lost (e.g. when you log out or close the web browser. And with it the running script. Because the training script regularly saves checkpoints, it can be restarted, but it would still require to have the client run for several days. In a non-virtual environment, you could just start the script, close the web browser window, and log in later to see how your training is going. Even if your terminal tab in JupyterLab is by accident closed, you can still bring it to the foreground by clicking on the terminal session in the \"Running Terminals and Kernels\" section on the left side of JupyterHub (under the icon for the File Browser). \n",
    "- Another alternative is to start the script in a background process from the start and just observe the training process in the log files. But that also does not work in the virtual environment that is automatically stopped when the connection is lost for some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "For evaluating the performance of the network and also to do some predictions, VoteNet provides the \"eval.py\" script. As with the training script, the eval script needs to be adapted in the same way.\n",
    "\n",
    "First, add the following code related to the ISPRS dataset after the ScanNet part and before the \"else\"-part (that prints out an error message in the next line). Since we currently do not have validation data, the \"train\" data is once used for demonstrating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif FLAGS.dataset == 'isprs':\n",
    "    sys.path.append(os.path.join(ROOT_DIR, 'ISPRS'))\n",
    "    from ISPRS_detection_dataset import ISPRSDetectionVotesDataset, MAX_NUM_OBJ\n",
    "    from model_util_ISPRS import ISPRSDatasetConfig\n",
    "    DATASET_CONFIG = ISPRSDatasetConfig()\n",
    "#    TEST_DATASET = ISPRSDetectionVotesDataset('val', num_points=NUM_POINT,\n",
    "    TEST_DATASET = ISPRSDetectionVotesDataset('train', num_points=NUM_POINT,\n",
    "        augment=False,\n",
    "        use_color=False, use_height=(not FLAGS.no_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although all data patches will be evaluated, only 1 patch is written as output. It is actually a complete batch that is written to files and not a patch. But with a batch size of 1, the batch id equals the patch id and it is easier to target a defined patch for output.\n",
    "\n",
    "You can change the batch_idx to a patch that might interest you. Find the the following line and change the id of 0 to any other number, like 380."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if batch_idx == 0:\n",
    "            MODEL.dump_results(end_points, DUMP_DIR, DATASET_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the \"eval.py\" script file and run it within the terminal window from the \"votenet\" directory with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 python eval.py --dataset isprs --checkpoint_path log_isprs/checkpoint.tar --dump_dir eval_isprs --cluster_sampling seed_fps --use_3d_nms --use_cls_nms --per_class_proposal --num_point 100000 --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution: The command in the above code cell is very long and it must be provided as one. If you copy & paste it with a line break, then your command might not be complete and you will get an error message.** Better copy and paste it first into some text editor, make sure it is a one-liner, remove any line breaks if there are any, and then copy it into the terminal window.\n",
    "\n",
    "**The following arguments are the more important once for running the eval script:**\n",
    "\n",
    "- The path where the checkpoint is located (with --checkpoint_path). For example \"log_isprs\" for the checkpoint of your trained model.\n",
    "\n",
    "- The argument --dump_dir specifies where the output files are written to.\n",
    "\n",
    "Check the \"eval.py\" file for the other arguments or get the help message from the script itself with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: eval.py [-h] [--model MODEL] [--dataset DATASET]\n",
      "               [--checkpoint_path CHECKPOINT_PATH] [--dump_dir DUMP_DIR]\n",
      "               [--num_point NUM_POINT] [--num_target NUM_TARGET]\n",
      "               [--batch_size BATCH_SIZE] [--vote_factor VOTE_FACTOR]\n",
      "               [--cluster_sampling CLUSTER_SAMPLING]\n",
      "               [--ap_iou_thresholds AP_IOU_THRESHOLDS] [--no_height]\n",
      "               [--use_color] [--use_sunrgbd_v2] [--use_3d_nms] [--use_cls_nms]\n",
      "               [--use_old_type_nms] [--per_class_proposal] [--nms_iou NMS_IOU]\n",
      "               [--conf_thresh CONF_THRESH] [--faster_eval] [--shuffle_dataset]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         Model file name [default: votenet]\n",
      "  --dataset DATASET     Dataset name. sunrgbd or scannet. [default: sunrgbd]\n",
      "  --checkpoint_path CHECKPOINT_PATH\n",
      "                        Model checkpoint path [default: None]\n",
      "  --dump_dir DUMP_DIR   Dump dir to save sample outputs [default: None]\n",
      "  --num_point NUM_POINT\n",
      "                        Point Number [default: 20000]\n",
      "  --num_target NUM_TARGET\n",
      "                        Point Number [default: 256]\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch Size during training [default: 8]\n",
      "  --vote_factor VOTE_FACTOR\n",
      "                        Number of votes generated from each seed [default: 1]\n",
      "  --cluster_sampling CLUSTER_SAMPLING\n",
      "                        Sampling strategy for vote clusters: vote_fps,\n",
      "                        seed_fps, random [default: vote_fps]\n",
      "  --ap_iou_thresholds AP_IOU_THRESHOLDS\n",
      "                        A list of AP IoU thresholds [default: 0.25,0.5]\n",
      "  --no_height           Do NOT use height signal in input.\n",
      "  --use_color           Use RGB color in input.\n",
      "  --use_sunrgbd_v2      Use SUN RGB-D V2 box labels.\n",
      "  --use_3d_nms          Use 3D NMS instead of 2D NMS.\n",
      "  --use_cls_nms         Use per class NMS.\n",
      "  --use_old_type_nms    Use old type of NMS, IoBox2Area.\n",
      "  --per_class_proposal  Duplicate each proposal num_class times.\n",
      "  --nms_iou NMS_IOU     NMS IoU threshold. [default: 0.25]\n",
      "  --conf_thresh CONF_THRESH\n",
      "                        Filter out predictions with obj prob less than it.\n",
      "                        [default: 0.05]\n",
      "  --faster_eval         Faster evaluation by skippling empty bounding box\n",
      "                        removal.\n",
      "  --shuffle_dataset     Shuffle the dataset (random order).\n"
     ]
    }
   ],
   "source": [
    "!python votenet/eval.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output folder (eval_isprs), you find quite a number of files. CloudCompare should be able to load and visualize them all. Most interesting are the input point cloud (\\_pc.ply), the predicted bounding boxes with high confidence without (or before) (\\_pred\\_confident\\_bbox.ply) and with (or after) non-maximum suppression (\\_pred\\_confident\\_nms\\_bbox.ply). There are also files with the ground truth data (\\_gt\\_) that might be interesting to compare the predicted bounding boxes against.\n",
    "\n",
    "Depending on how many epochs you trained, the results look more or less promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-trained model is provided in the coursematerial folder that can be used for evaluation and also to visually compare it to the model that you trained. Do one more evaluation run with the pre-trained model and compare the evaluation metrics with the one you trained. Also check out and compare the outputs of this model with what you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 python eval.py --dataset isprs --checkpoint_path /home/jovyan/coursematerial/GIS/ISPRS/VoteNet/pretrained_model/checkpoint.tar --dump_dir eval_isprs --cluster_sampling seed_fps --use_3d_nms --use_cls_nms --per_class_proposal --num_point 100000 --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing a model trained for only a few dozens epochs (or even 100 or 200 epochs) with the provided pre-trained model that was trained for 400 epochs, you should see a huge decrease in the mean overall loss (e.g. 600 down to 125), and an increase in the mean average precision as high as 0.82 (IoU 0.25 in NMS) and 0.53 (IoU 0.50 in NMS). \n",
    "\n",
    "**But one has to remember that evaluation was performed on the training data! So, the metrics do not say anything about how the network performs on unseen data. The model might be completely overfitted and might not predict anything meaningful at all for unseen data.**)\n",
    "\n",
    "This concludes the training and evaluation part of this exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}