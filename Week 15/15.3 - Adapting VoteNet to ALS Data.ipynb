{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.3 - Adapting VoteNet to ALS Data\n",
    "\n",
    "This notebook shows how to adapt VoteNet for its application on ALS point cloud data. This mainly concerns the change of the hyperparameters of the layers of the neural network model. Another small aspect is the weighting of the individual parts of the multi-task loss function.\n",
    "\n",
    "In this notebook, individual files of the cloned GitHub repository that are located in the \"models\" folder are changed for this purpose, of which we have made backup copies in the first notebook. Just double click in the file browser of JupyterLab on the respective Python files. After you have made modifications to a Python file, make sure to save it with \"Save Python File\" in the menu File before you use it for training the model in the terminal window. (Unfortunately, Python files are not automatically saved and there is also no convenient button above the editor.)\n",
    "\n",
    "**Disclaimer: Please be aware that the exercise on VoteNet as well as the provided files are the result of a rather quick hack and are probably not free of errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone Module\n",
    "\n",
    "The backbone of the network is the PointNet++ module for feature learning and extraction, which also outputs the seed points that are used in the remainder of VoteNet. (See the lecture notes for the complete VoteNet architecture.) Open the Python file \"backbone_module.py\" and study it for a moment. \n",
    "\n",
    "In the *\\_\\_init\\_\\_()* constructor method, the layers of the PointNet++ module are constructed and their hyperparameters set. These hyperparameter values have been chosen for indoor scenes, where the extents of the objects and the scene itself are much smaller than in urban scenes with buildings, trees, cars, streets, etc. \n",
    "\n",
    "The second major method is the *forward()* method that is used in the forward pass of network training. Here, you can see how the outputs of one layer (x,y,z-coordinates and features) are forwarded to the next layer. Since PyTorch keeps track of how the computation is performed in the forward pass, it can automatically derive from the forward pass the computations of the backwards pass. Therefore, the backwards pass does not need to be provided by the programmer. \n",
    "\n",
    "(In the dictionary called \"end_points\", VoteNet collects the different outputs of all the layers for further use and debugging. This dictionary also contains a lot of data that is not really needed when operating the network. And one could implement the VoteNet network also without the many lines of code in which the variable \"end_points\" occur.)\n",
    "\n",
    "To adapt the backbone module, perform the following changes:\n",
    "\n",
    "**Change the hyperparameters of the four set abstraction layers (PointnetSAModuleVotes) to the following values:**\n",
    "\n",
    "Layer 1: Number of points: 8192, radius:  1.0, number of samples: 16, mlp: input_feature_dim, 64, 64, 128.  \n",
    "Layer 2: Number of points: 4096, radius:  5.0, number of samples: 64, mlp: 128, 128, 256.  \n",
    "Layer 3: Number of points: 2048, radius: 15.0, number of samples: 64, mlp: 256, 128, 256.  \n",
    "Layer 4: Number of points:  512, radius: 20.0, number of samples: 32, mlp: 256, 128, 256.\n",
    "\n",
    "(Leave the parameters \"use_xyz\" and \"normalize_xyz\" to the Boolean value True.)\n",
    "\n",
    "**Change the hyperparameters of the two feature propagation layers (PointnetFPModule) to the following values:**\n",
    "\n",
    "Layer 1: mlp: 256+256, 256  \n",
    "Layer 2: mlp: 256+256, 256\n",
    "\n",
    "The parameter values were chosen following the PointNet++ implementation for ALS data. After you have trained the network once and evaluated the quality metrics, feel free to choose other hyperparameters and try to find better ones. You can also try to use only 3 set abstraction layers, but then remember that the output from the last set abstraction layer is passed into the first feature propagation layer. In addition, the number of feature propagation layers must then also be reduced, so as not to generate too many seed points. Note that the feature propagation layers restore the number of points that are sampled in the set abstraction layers and that these are the seed points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal Radius\n",
    "\n",
    "In the second part of the VoteNet architecture, the vote points are clustered to get a fixed number (defined by a hyperparameter) of object centers as object proposals. For this clustering process, a radius for a spherical neighborhood must be specified. This is necessary as the network is not capable to predict the coordinates of vote points that vote for the same object to be exactly at the same position. There will rather be some deviations from it and the clustering with a certain radius will compensate for it.\n",
    "\n",
    "For indoor scenes, the clustering radius is set to 0.3m, which might be sufficient for smaller objects (of sizes 1m to 3m) like furniture. However, it is too small for larger outdoor objects like buildings or trees. \n",
    "\n",
    "The clustering of vote points in the object proposal module of VoteNet is implemented as a special set abstraction layer called \"PointnetSAModuleVotes\". Open the \"proposal_module.py\" file and look for this layer in constructor (*\\_\\_init()\\_\\_*) of the \"ProposalModule\" class.\n",
    "\n",
    "**Change the radius of the set abstraction layer (PointnetSAModuleVotes) of the vote module to 5.0m.**\n",
    "\n",
    "A radius of 5.0m should be sufficient to cluster the vote points for larger objects like buildings. (If the radius is chosen too large, then the votes for several objects might be clustered to one object proposal. Therefore, the clustering radius should also not be chosen too large.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "The loss function consists of several parts (multi-task loss). One of its parts is the objectness loss, where the positions of the predicted object centers are compared with the true object centers. As a simplified explanation, there are two thresholds: if the predicted center of an object is closer than NEAR_THRESHOLD from the center of a ground truth object, the prediction gets an objectness label of 1 (and otherwise a label of 0). However, not all predictions actually contribute to the loss function. Only votes that are correctly predicted as objects and votes that should be predict as object, but are farther located than FAR_THRESHOLD, contribute to the loss value. Votes in between NEAR_THRESHOLD and FAR_THRESHOLD are not considered to be wrong with respect of the loss function.\n",
    "\n",
    "The default threshold values are again for indoor scenes and defined as 0.3m and 0.6m for the near and far thresholds. These values are obviously too small for outdoor scenes.\n",
    "\n",
    "Open the file \"loss_helper.py\" and **change the thresholds of NEAR_THRESHOLD to 5.0m and FAR_THRESHOLD to 10.0m.**\n",
    "\n",
    "The multi-task loss weights the different parts of the loss function with a number of hyperparameters. From experiments, we noticed better results when increasing the influence of the heading classification in the loss for the oriented bounding boxes (box loss). \n",
    "\n",
    "**Find the code line that calculates the box_loss (box_loss = ...) in the *get_loss()* function and change the weight of the heading class loss (heading_cls_loss) from 0.1 to 1.0.** This should improve the prediction of the orientation class of the oriented bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final words:**\n",
    "\n",
    "This concludes the changes that are needed to adapt VoteNet to ALS data. Once more, make sure you saved all the modified Python files with \"Save Python File\" from the File menu.\n",
    "\n",
    "**Continue now with the next notebook (15.4).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}